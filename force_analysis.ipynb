{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets.augmentations import Compose, CentreCrop, SquareResize, Normalize, ArrayToTensor\n",
    "from models.force_estimator_2d import ForceEstimatorV, ForceEstimatorVS\n",
    "from models.force_estimator_transformers import ViT\n",
    "from path import Path\n",
    "from datasets.vision_state_dataset import normalize_labels, load_as_float\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folder to be analysed\n",
    "folder_dir = \"/home/md21local/test_force_visu_data/eco_30_pink_dragon_20_pink_double-layer_push\"\n",
    "folder_dir = Path(folder_dir)\n",
    "\n",
    "labels = np.array(pd.read_csv(folder_dir/\"labels.csv\"))\n",
    "norm_labels = normalize_labels(labels)\n",
    "\n",
    "forces = labels[:, -6:]\n",
    "\n",
    "inputs = []\n",
    "n_labels = labels.shape[0] // len(folder_dir.files('*.png'))\n",
    "step = 7\n",
    "\n",
    "for i, img_file in enumerate(sorted(folder_dir.files('*.png'))):\n",
    "    if i == 1220:\n",
    "        break\n",
    "\n",
    "    if i > 20:\n",
    "        data = {}\n",
    "        img = load_as_float(img_file)\n",
    "        state = norm_labels[i*n_labels:(i*n_labels) + step, :-6]\n",
    "        force = labels[i*n_labels:(i*n_labels) + step, -6:]\n",
    "        data['img'] = img\n",
    "        data['state'] = state\n",
    "        data['force'] = 0.1 * force\n",
    "        inputs.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the transformation for the images\n",
    "normalize = Normalize(mean = [0.45, 0.45, 0.45],\n",
    "                    std = [0.225, 0.225, 0.225])\n",
    "transform = Compose(\n",
    "    [CentreCrop(), \n",
    "    SquareResize(),\n",
    "    ArrayToTensor(),\n",
    "    normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model you want to test, uncommet for it\n",
    "\n",
    "# VISION ONLY CNN #\n",
    "cnn_visual = ForceEstimatorV(final_layer=6)\n",
    "cnn_visual_checkpoint = torch.load('/home/md21local/mreyzabal/checkpoints/img2force/cnn/visu/01-27-09:50/checkpoint.pth.tar')\n",
    "cnn_visual.load_state_dict(cnn_visual_checkpoint['state_dict'], strict=False)\n",
    "\n",
    "# VISION AND STATE CNN #\n",
    "cnn_state = ForceEstimatorVS(rs_size=25, final_layer=30)\n",
    "cnn_state_checkpoint = torch.load('/home/md21local/mreyzabal/checkpoints/img2force/cnn/visu_state/01-27-09:50/checkpoint.pth.tar')\n",
    "cnn_state.load_state_dict(cnn_state_checkpoint['state_dict'], strict=False)\n",
    "\n",
    "cnn_visual.eval()\n",
    "cnn_state.eval()\n",
    "\n",
    "# VISION ONLY VIT #\n",
    "# vit_visual = ViT(\n",
    "#             image_size = 256,\n",
    "#             patch_size = 16,\n",
    "#             num_classes = 6,\n",
    "#             dim = 1024,\n",
    "#             depth = 6,\n",
    "#             heads = 16,\n",
    "#             mlp_dim = 2048,\n",
    "#             dropout = 0.1,\n",
    "#             emb_dropout = 0.1,\n",
    "#             max_tokens_per_depth=(256, 128, 64, 32, 16, 8),\n",
    "#             state_include = False\n",
    "# )\n",
    "# vit_visual_checkpoint = torch.load('/home/md21local/mreyzabal/checkpoints/img2force/vit/visu/01-27-10:36/checkpoint.pth.tar')\n",
    "# vit_visual.load_state_dict(vit_visual_checkpoint['state_dict'], strict=False)\n",
    "\n",
    "# # VISION AND STATE VIT #\n",
    "# vit_state = ViT(\n",
    "#             image_size = 256,\n",
    "#             patch_size = 16,\n",
    "#             num_classes = 6,\n",
    "#             dim = 1024,\n",
    "#             depth = 6,\n",
    "#             heads = 16,\n",
    "#             mlp_dim = 2048,\n",
    "#             dropout = 0.1,\n",
    "#             emb_dropout = 0.1,\n",
    "#             max_tokens_per_depth=(256, 128, 64, 32, 16, 8),\n",
    "#             state_include = True\n",
    "# )\n",
    "# vit_state_checkpoint = torch.load('/home/md21local/mreyzabal/checkpoints/img2force/vit/visu_state/01-27-09:50/checkpoint.pth.tar')\n",
    "# vit_state.load_state_dict(vit_state_checkpoint['state_dict'], strict=False)\n",
    "\n",
    "# vit_state.eval()\n",
    "# vit_visual.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visu_forces = []\n",
    "state_forces = []\n",
    "visu_rmse = []\n",
    "state_rmse = []\n",
    "forces = []\n",
    "\n",
    "for i, inp in enumerate(inputs):\n",
    "    if i == 600: \n",
    "        break\n",
    "    state = torch.from_numpy(inp['state']).unsqueeze(0).float()\n",
    "    force = torch.from_numpy(inp['force']).unsqueeze(0).float()\n",
    "    img = transform([inp['img']])[0]\n",
    "\n",
    "    pred_force_visu = cnn_visual(img.unsqueeze(0))\n",
    "    pred_force_state = torch.zeros(*force.shape)\n",
    "    for i in range(pred_force_state.shape[1]):\n",
    "        s = state[:, i, :]\n",
    "        pred_force_state[:, i, :] = cnn_state(img.unsqueeze(0), s)\n",
    "\n",
    "    rmse_visu = torch.sqrt(((force.mean(axis=1) - pred_force_visu) ** 2).mean())\n",
    "    rmse_state = torch.sqrt(((force - pred_force_state) ** 2).mean())\n",
    "\n",
    "    visu_rmse.append(rmse_visu.item())\n",
    "    state_rmse.append(rmse_state.item())\n",
    "    visu_forces.append(pred_force_visu.squeeze(0).detach().numpy())\n",
    "    state_forces.append([f for f in pred_force_state.squeeze(0).detach().numpy()])\n",
    "    forces.append([f for f in force.squeeze(0).detach().numpy()])\n",
    "\n",
    "visu_rmse = np.array(visu_rmse)\n",
    "state_rmse = np.array(state_rmse)\n",
    "visu_forces = np.array(visu_forces)\n",
    "state_forces = np.array(state_forces).reshape(-1, 6)\n",
    "forces = np.array(forces).reshape(-1, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_x = (1/30)*np.linspace(0, len(visu_forces), len(state_forces))\n",
    "visu_x = (1/30)*np.linspace(0, len(visu_forces), len(visu_forces))\n",
    "\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(state_x, forces[:, 0], 'b')\n",
    "# plt.plot(visu_x, visu_forces[:, 0], 'r')\n",
    "plt.plot(state_x, state_forces[:, 0], 'g')\n",
    "plt.legend([\"Ground truth\", \"VSCNN\"])\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Force X (N)\")\n",
    "plt.title(\"Force estimation\")\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(state_x, forces[:, 1], 'b')\n",
    "# plt.plot(visu_x, visu_forces[:, 1], 'r')\n",
    "plt.plot(state_x, state_forces[:, 1], 'g')\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Force Y (N)\")\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(state_x, forces[:, 2], 'b')\n",
    "# plt.plot(visu_x, visu_forces[:, 2], 'r')\n",
    "plt.plot(state_x, state_forces[:, 2], 'g')\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Force Z (N)\")\n",
    "fig.savefig('figures/pre-results_cnn.png', dpi=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = ['VCNN', 'VSCNN']\n",
    "rmse_values = [visu_rmse.mean(), state_rmse.mean()]\n",
    "fig = plt.figure(figsize = (10, 5))\n",
    "plt.bar(name[0], rmse_values[0], color='red', width=0.5)\n",
    "plt.bar(name[1], rmse_values[1], color='green', width=0.5)\n",
    "plt.xlabel('Network architectures')\n",
    "plt.ylabel('RMSE (N)')\n",
    "plt.title('Error')\n",
    "fig.savefig('figures/pre-errors_cnn.png', dpi=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "save_dic = {'visu_rmse': visu_rmse, 'state_rmse': state_rmse, 'visu_force': visu_forces, 'state_force': state_forces}\n",
    "f = open(\"results/cnn.pkl\", \"wb\")\n",
    "pickle.dump(save_dic, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "cnn_file = open('results/cnn.pkl', 'rb')\n",
    "cnn_data = pickle.load(cnn_file)\n",
    "\n",
    "vit_file = open('results/transformers.pkl', 'rb')\n",
    "vit_data = pickle.load(vit_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_state_force = vit_data['state_force']\n",
    "cnn_state_force = cnn_data['state_force']\n",
    "\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(state_x, forces[:, 0], 'b')\n",
    "plt.plot(state_x, cnn_state_force[:, 0], 'r')\n",
    "plt.plot(state_x, vit_state_force[:, 0], 'g')\n",
    "plt.legend([\"Ground truth\", \"VSCNN\", \"ViST\"])\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Force X (N)\")\n",
    "plt.title(\"Force estimation\")\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(state_x, forces[:, 1], 'b')\n",
    "plt.plot(state_x, cnn_state_force[:, 1], 'r')\n",
    "plt.plot(state_x, vit_state_force[:, 1], 'g')\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Force Y (N)\")\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(state_x, forces[:, 2], 'b')\n",
    "plt.plot(state_x, cnn_state_force[:, 2], 'r')\n",
    "plt.plot(state_x, vit_state_force[:, 2], 'g')\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Force Z (N)\")\n",
    "fig.savefig('figures/pre-results.png', dpi=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a video with the force estimation and the video\n",
    "\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "state_x = np.linspace(0, 600, len(cnn_state_force))\n",
    "\n",
    "gs = GridSpec(2, 1, height_ratios=[4, 1])\n",
    "ax1 = fig.add_subplot(gs[0])\n",
    "ax2 = fig.add_subplot(gs[1])\n",
    "ax2.set_xlabel(\"Time (s)\")\n",
    "ax2.set_ylabel(\"Force (N)\")\n",
    "\n",
    "force = np.array([s['force'] for s in inputs[:600]]).reshape(-1, 6)\n",
    "\n",
    "def animate(i):\n",
    "    ax1.imshow((255. * inputs[i]['img']).astype(np.uint8))\n",
    "    ax1.set_title(\"Video\")\n",
    "    ax2.plot(state_x[:(i+1)*7], force[:(i+1)*7, 0], 'b')\n",
    "    ax2.plot(state_x[:(i+1)*7], cnn_state_force[:(i+1)*7, 0], 'r')\n",
    "    ax2.plot(state_x[:(i+1)*7], vit_state_force[:(i+1)*7, 0], 'g')\n",
    "    ax2.legend(['Ground truth', 'VSCNN', 'ViST'])\n",
    "\n",
    "ani = animation.FuncAnimation(\n",
    "    fig, animate, frames=50, interval=500)\n",
    "\n",
    "ani.save(\"results/force_pred.gif\", dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "be17f01356bc3fe61b2f98816b02e4e5e5b547bfe2b5c8974c8f0394e0dd0b66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
